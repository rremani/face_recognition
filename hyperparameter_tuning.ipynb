{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "\n",
    "TRAIN_PATH = \"ibug_300W_large_face_landmark_dataset/labels_ibug_300W_train_eyes.xml\"\n",
    "TEST_PATH = \"ibug_300W_large_face_landmark_dataset/labels_ibug_300W_test_eyes.xml\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# define the path to the temporary model file\n",
    "TEMP_MODEL_PATH = \"temp.dat\"\n",
    "# define the path to the output CSV file containing the results of\n",
    "# our experiments\n",
    "CSV_PATH = \"trials.csv\"\n",
    "# define the path to the example image we'll be using to evaluate\n",
    "# inference speed using the shape predictor\n",
    "IMAGE_PATH = \"example.jpg\"\n",
    "# define the number of threads/cores we'll be using when trianing our\n",
    "# shape predictor models\n",
    "PROCS = -1\n",
    "# define the maximum number of trials we'll be performing when tuning\n",
    "# our shape predictor hyperparameters\n",
    "MAX_TRIALS = 100"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# import the necessary packages\n",
    "# from pyimagesearch import config\n",
    "from sklearn.model_selection import ParameterGrid\n",
    "import multiprocessing\n",
    "import numpy as np\n",
    "import random\n",
    "import time\n",
    "import dlib\n",
    "import cv2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def evaluate_model_acc(xmlPath, predPath):\n",
    "    # compute and return the error (lower is better) of the shape\n",
    "    # predictor over our testing path\n",
    "    return dlib.test_shape_predictor(xmlPath, predPath)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def evaluate_model_speed(predictor, imagePath, tests=10):\n",
    "    # initialize the list of timings\n",
    "    timings = []\n",
    "    # loop over the number of speed tests to perform\n",
    "    for i in range(0, tests):\n",
    "        # load the input image and convert it to grayscale\n",
    "        image = cv2.imread(config.IMAGE_PATH)\n",
    "        gray = cv2.cvtColor(image, cv2.COLOR_BGR2GRAY)\n",
    "        # detect faces in the grayscale frame\n",
    "        detector = dlib.get_frontal_face_detector()\n",
    "        rects = detector(gray, 1)\n",
    "        # ensure at least one face was detected\n",
    "        if len(rects) > 0:\n",
    "            # time how long it takes to perform shape prediction\n",
    "            # using the current shape prediction model\n",
    "            start = time.time()\n",
    "            shape = predictor(gray, rects[0])\n",
    "            end = time.time()\n",
    "        # update our timings list\n",
    "        timings.append(end - start)\n",
    "    # compute and return the average over the timings\n",
    "    return np.average(timings)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# define the columns of our output CSV file\n",
    "cols = [\n",
    "\t\"tree_depth\",\n",
    "\t\"nu\",\n",
    "\t\"cascade_depth\",\n",
    "\t\"feature_pool_size\",\n",
    "\t\"num_test_splits\",\n",
    "\t\"oversampling_amount\",\n",
    "\t\"oversampling_translation_jitter\",\n",
    "\t\"inference_speed\",\n",
    "\t\"training_time\",\n",
    "\t\"training_error\",\n",
    "\t\"testing_error\",\n",
    "\t\"model_size\"\n",
    "]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# open the CSV file for writing and then write the columns as the\n",
    "# header of the CSV file\n",
    "\n",
    "csv = open(CSV_PATH, \"w\")\n",
    "csv.write(\"{}\\n\".format(\",\".join(cols)))\n",
    "# determine the number of processes/threads to use\n",
    "procs = multiprocessing.cpu_count()\n",
    "procs = PROCS if PROCS > 0 else procs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "# initialize the list of dlib shape predictor hyperparameters that\n",
    "# we'll be tuning over\n",
    "hyperparams = {\n",
    "    \"tree_depth\": list(range(2, 8, 2)),\n",
    "    \"nu\": [0.01, 0.1, 0.25],\n",
    "    \"cascade_depth\": list(range(6, 16, 2)),\n",
    "    \"feature_pool_size\": [100, 250, 500, 750, 1000],\n",
    "    \"num_test_splits\": [20, 100, 300],\n",
    "    \"oversampling_amount\": [1, 20, 40],\n",
    "    \"oversampling_translation_jitter\": [0.0, 0.1, 0.25]\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[INFO] sampling 100 of 6075 possible combinations\n"
     ]
    }
   ],
   "source": [
    "# construct the set of hyperparameter combinations and randomly\n",
    "# sample them as trying to test *all* of them would be\n",
    "# computationally prohibitive\n",
    "combos = list(ParameterGrid(hyperparams))\n",
    "random.shuffle(combos)\n",
    "sampledCombos = combos[:MAX_TRIALS]\n",
    "print(\"[INFO] sampling {} of {} possible combinations\".format(\n",
    "    len(sampledCombos), len(combos)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# loop over our hyperparameter combinations\n",
    "for (i, p) in enumerate(sampledCombos):\n",
    "    # log experiment number\n",
    "    print(\"[INFO] starting trial {}/{}...\".format(i + 1,len(sampledCombos)))\n",
    "\n",
    "    # grab the default options for dlib's shape predictor and then\n",
    "    # set the values based on our current hyperparameter values\n",
    "    options = dlib.shape_predictor_training_options()\n",
    "    options.tree_depth = p[\"tree_depth\"]\n",
    "    options.nu = p[\"nu\"]\n",
    "    options.cascade_depth = p[\"cascade_depth\"]\n",
    "    options.feature_pool_size = p[\"feature_pool_size\"]\n",
    "    options.num_test_splits = p[\"num_test_splits\"]\n",
    "    options.oversampling_amount = p[\"oversampling_amount\"]\n",
    "    otj = p[\"oversampling_translation_jitter\"]\n",
    "    options.oversampling_translation_jitter = otj\n",
    "    # tell dlib to be verbose when training and utilize our supplied\n",
    "    # number of threads when training\n",
    "    options.be_verbose = True\n",
    "    options.num_threads = procs\n",
    "    \n",
    "    \n",
    "    start = time.time()\n",
    "    dlib.train_shape_predictor(TRAIN_PATH,TEMP_MODEL_PATH, options)\n",
    "    trainingTime = time.time() - start\n",
    "    break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
